{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16RrIKxlL8hfUDjLTsuny8h064eEGQvl6","timestamp":1686664948773}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset\n","\n","df = pd.DataFrame({'a': [0,1,2], 'b': [3,4,5]})\n","dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n","\n","### convert to Huggingface dataset\n","hg_dataset = Dataset(pa.Table.from_pandas(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"bxAt9_voeYi2","executionInfo":{"status":"error","timestamp":1686856626608,"user_tz":-120,"elapsed":259,"user":{"displayName":"Kevin Chok","userId":"14369447446424477257"}},"outputId":"17f9da98-bee8-4392-a914-f9658a948384"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3e1ec9b8ebce>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m### convert to Huggingface dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhg_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sn0S2ogcec0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksFszljurvD8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n"]},{"cell_type":"code","source":["import os\n","for dirname, _, filenames in os.walk('/content/drive/MyDrive/Demoday_Project/Dataset/BBC_news/BBC_News_Summary/BBC_News_Summary'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"metadata":{"id":"ULrPPqmaukfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"5s1LSLOLuo5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLwZ8A43suw0","executionInfo":{"status":"ok","timestamp":1686743743983,"user_tz":-120,"elapsed":17733,"user":{"displayName":"Batch G","userId":"04279360399092331116"}},"outputId":"efb94245-d2b8-493a-b833-373a97a1c658"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","import tensorflow as tf\n","import tensorflow.keras"],"metadata":{"id":"t0-wn6XatBvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_split = 0.2\n","batch_size = 64\n","epochs = 2\n","learning_rate = 1e-4"],"metadata":{"id":"DB5G53kUtK5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sorted function\n","dataset_path = '/content/drive/MyDrive/Demoday_Project/Dataset/BBC_news/BBC_News_Summary/BBC_News_Summary'\n","articles_path = sorted(glob.glob(os.path.join(dataset_path, \"News Articles/*/*.txt\")))\n","summaries_path = sorted(glob.glob(os.path.join(dataset_path, \"Summaries/*/*.txt\")))\n","print(articles_path[3], summaries_path[3])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"nssIg-tZuYTC","executionInfo":{"status":"error","timestamp":1686743749568,"user_tz":-120,"elapsed":714,"user":{"displayName":"Batch G","userId":"04279360399092331116"}},"outputId":"78979b0d-02c0-4910-e414-3ad0ea1c1666"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-46198b29f589>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticles_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"News Articles/*/*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummaries_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Summaries/*/*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["articles = []\n","summaries = []\n","\n","for article_path, summary_path in zip(articles_path, summaries_path):\n","    with open(article_path, \"r\", encoding=\"ISO-8859-1\") as article_file:\n","        article_text = article_file.read()\n","        articles.append(article_text)\n","\n","    with open(summary_path, \"r\", encoding=\"ISO-8859-1\") as summary_file:\n","        summary_text = summary_file.read()\n","        summaries.append(summary_text)"],"metadata":{"id":"_RZ0Yy3nu1Rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categories_list = os.listdir(os.path.join(dataset_path, 'News Articles'))\n","categories = []\n","def print_file_data():\n","    for category in categories_list:\n","        article_paths = glob.glob(os.path.join(dataset_path, \"News Articles\", category, \"*.txt\"))\n","        summary_paths = glob.glob(os.path.join(dataset_path, \"Summaries\", category, \"*.txt\"))\n","\n","        for i in article_paths:\n","            categories.append(category)\n","\n","        if len(article_paths) != len(summary_paths):\n","            print(\"Length of dataset not equal for subdirectory :\", category)\n","        else:\n","            print(f\"Found {len(article_paths)} articles and {len(summary_paths)} summaries in the subdirectory : {category}\")\n","\n","print_file_data()"],"metadata":{"id":"LvTtr4Piu-kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["articles[2]"],"metadata":{"id":"AX95FqBIvCzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summaries[2]"],"metadata":{"id":"Gx3--hg-vFQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame({\n","    'Articles' : articles,\n","    'Summaries' : summaries,\n","    #'Categories' : categories\n","})\n","\n","df.head()"],"metadata":{"id":"1TjxPXPrvIK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#category = df.groupby(\"Categories\").size()\n","#print(category)\n","#plt.figure(figsize=(10,6))\n","#sns.barplot(x=category.index, y=category)\n","#plt.title(\"Category wise distribution of Articles\")\n","#plt.show()"],"metadata":{"id":"mczsOGT4vLOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Article Length'] = df[\"Articles\"].apply(lambda x: len(x.split()))\n","df['Summary Length'] = df[\"Summaries\"].apply(lambda x: len(x.split()))"],"metadata":{"id":"B_qr6ssTvS0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"tjQDmyIdvTf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","from transformers import TFBartForConditionalGeneration, BartTokenizer"],"metadata":{"id":"jj4CaJYVvW8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n","model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"],"metadata":{"id":"q5jVn9JpvaFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer"],"metadata":{"id":"t1C3ryeIvdX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"zGLV93BbvnUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filters = '[!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n]'\n","df[\"Articles\"] = df[\"Articles\"].str.replace('.', ' ', regex=True)\n","df['Articles'] = df[\"Articles\"].str.replace(filters, '', regex=True)\n","df[\"Summaries\"] = df[\"Summaries\"].str.replace('.', ' ', regex=True)\n","df['Summaries'] = df[\"Summaries\"].str.replace(filters, '', regex=True)"],"metadata":{"id":"_P-1SsyLvprV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"Articles\"][3]"],"metadata":{"id":"UjksyOr4vsYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X = df[\"Articles\"]\n","Y = df[\"Summaries\"]\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","train_df = pd.concat([X_train, Y_train], axis=1)\n","test_df = pd.concat([X_test, Y_test], axis=1)"],"metadata":{"id":"5L9iQSlEvuVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df"],"metadata":{"id":"NjHmhoQ3vyac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print((X_train.shape, Y_train.shape), (X_test.shape, Y_test.shape))"],"metadata":{"id":"_vb0I4Nov0VP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import InputExample, InputFeatures\n","def convert_data_to_examples(train, test, data, label):\n","    train_input_examples = train.apply(lambda x: InputExample(\n","        guid = None,\n","        text_a = x[data],\n","        text_b = None,\n","        label = x[label]\n","    ), axis=1)\n","\n","    test_input_examples = test.apply(lambda x: InputExample(\n","        guid = None,\n","        text_a = x[data],\n","        text_b = None,\n","        label = x[label]\n","    ), axis=1)\n","\n","    return train_input_examples, test_input_examples"],"metadata":{"id":"U9XmEgGGv3eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_examples, test_input_examples = convert_data_to_examples(train_df, test_df, \"Articles\", \"Summaries\")"],"metadata":{"id":"qjowrPejv6m4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_examples"],"metadata":{"id":"PsjHEvU-v9r9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_input_examples"],"metadata":{"id":"UF076deRwBeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_examples[6]"],"metadata":{"id":"bmAwRx9RwCYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#'Run this cell to see how to tokenizer.encode_plus works'\n","\n","\n","# input_dict = tokenizer.encode_plus(\n","#             list(train_input_examples[6].text_a),\n","#             add_special_tokens = True, # adds <start> and <end> tokens\n","#             max_length = 128, # defining max_len for padding\n","#             return_token_type_ids = True, # return token_type_ids\n","#             pad_to_max_length = True, # pads the sentences shorter than max_len\n","#             return_attention_mask = True, # return attention mask (tells model if the padded token is actual sentence or just added for dimension coorection of instance)\n","#             truncation = True\n","#         )\n","# input_dict"],"metadata":{"id":"V1YDr4jiwHOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizing examples and converting into tensorflow dataset\n","def convert_examples_to_tf_dataset(examples, tokenizer, max_len=1024):\n","    features = [] #     Array to store the features\n","\n","    for example in examples:\n","        input_dict = tokenizer.encode_plus(\n","            list(example.text_a),\n","            add_special_tokens = True, # adds <start> and <end> tokens\n","            max_length = max_len, # defining max_len for padding\n","            return_token_type_ids = True, # return token_type_ids\n","            pad_to_max_length = True, # pads the sentences shorter than max_len\n","            return_attention_mask = True, # return attention mask (tells model if the padded token is actual sentence or just added for dimension coorection of instance)\n","            truncation = True\n","        )\n","#       Returning all the params\n","        input_ids, attention_masks = (input_dict[\"input_ids\"], input_dict[\"attention_mask\"])\n","\n","\n","\n","#         Same processing for labels\n","        label_dict = tokenizer.encode_plus(\n","            list(example.label),\n","            add_special_tokens = True, # adds <start> and <end> tokens\n","            max_length = max_len, # defining max_len for padding\n","            return_token_type_ids = True, # return token_type_ids\n","            pad_to_max_length = True, # pads the sentences shorter than max_len\n","            return_attention_mask = True, # return attention mask (tells model if the padded token is actual sentence or just added for dimension coorection of instance)\n","            truncation = True\n","        )\n","\n","        label_ids = label_dict[\"input_ids\"]\n","\n","#       Adding these params in the features array to feed into the model later\n","        features.append(\n","            InputFeatures(\n","                input_ids = input_ids, attention_mask = attention_masks, label=label_ids\n","            )\n","        )\n","\n","\n","        def gen():\n","            for f in features:\n","                yield(\n","                    {\n","                        'input_ids':f.input_ids,\n","                        'attention_mask':f.attention_mask,\n","                    },\n","                    f.label[0],\n","                )\n","\n","\n","\n","#     Returning data as tensroflow dataset for easy input in our model\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int32),\n","        (\n","            {\n","                \"input_ids\":tf.TensorShape([None]),\n","                \"attention_mask\":tf.TensorShape([None]),\n","            },\n","            tf.TensorShape([]),\n","        ),\n","    )\n"],"metadata":{"id":"I-_023VIwSTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preparing the dataset\n","train_data = convert_examples_to_tf_dataset(list(train_input_examples), tokenizer)\n","test_data = convert_examples_to_tf_dataset(list(train_input_examples), tokenizer)"],"metadata":{"id":"i3gd2H8rwzEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.take(0)"],"metadata":{"id":"e8jiT5csVfSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data"],"metadata":{"id":"GEbuipPNUjBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preparing shuffles batches\n","train_data = train_data.shuffle(100).batch(32)\n","test_data = test_data.batch(32)"],"metadata":{"id":"qt5UNWl_w1Cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data"],"metadata":{"id":"lImmlxo_w1uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data"],"metadata":{"id":"E8v5PY4nw31u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataset_count(dataset):\n","    count = 0\n","    for _ in dataset:\n","        count+=1\n","\n","    return count\n","\n","print(dataset_count(train_data))\n","print(dataset_count(test_data))"],"metadata":{"id":"4Q_yxDZfw7N5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.optimizers import Adam\n","from keras.losses import SparseCategoricalCrossentropy\n","from keras.metrics import SparseCategoricalAccuracy\n","model.compile(Adam(learning_rate=learning_rate),loss=SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[SparseCategoricalAccuracy('accuracy')])"],"metadata":{"id":"bRVnNEhTw77b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(train_data, epochs=epochs, validation_data=test_data)"],"metadata":{"id":"_BCjLQbKw-7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PADW5F09XHSB"},"execution_count":null,"outputs":[]}]}